{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train dataset has 60000 images.\n",
      "MNIST test  dataset has 10000 images.\n"
     ]
    }
   ],
   "source": [
    "# ALI BABOLHAVAEJI\n",
    "# 7/6/2019\n",
    "\n",
    "from torchvision import datasets , transforms\n",
    "from torch.utils.data import *\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "train_data=datasets.MNIST(root='MNIST_dataset',\n",
    "                          train=True,\n",
    "                          download=False,\n",
    "                          transform=transform)\n",
    "\n",
    "test_data=datasets.MNIST(root='MNIST_dataset',\n",
    "                         train=False,\n",
    "                         download=False,\n",
    "                         transform=transform)\n",
    "\n",
    "print('MNIST train dataset has {} images.'.format(len(train_data)))\n",
    "print('MNIST test  dataset has {} images.'.format(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_data(train_data,\n",
    "                     test_data=None,\n",
    "                     batch_size=20,\n",
    "                     num_workers=0,\n",
    "                     valid_size=0.2,\n",
    "                     sampler=SubsetRandomSampler):\n",
    "    \n",
    "    num_train=len(train_data)\n",
    "    indices= list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split=int(np.floor(valid_size*num_train))\n",
    "#     print(split)\n",
    "    train_idx,valid_idx=indices[split:],indices[:split]\n",
    "    train_sampler=sampler(train_idx)\n",
    "    valid_sampler=sampler(valid_idx)\n",
    "    \n",
    "    if test_data is not None:\n",
    "        test_loader= DataLoader(test_data, batch_size=batch_size,\n",
    "                                num_workers=num_workers) \n",
    "    else:\n",
    "        train_idx,test_idx= train_idx[split:],train_idx[:split]\n",
    "        train_sampler=sampler(train_idx)\n",
    "        test_sampler =sampler(test_idx)\n",
    "\n",
    "        test_loader= DataLoader(train_data, batch_size=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 sampler=test_sampler)\n",
    "        \n",
    "    train_loader= DataLoader(train_data, batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             sampler=train_sampler)\n",
    "    \n",
    "    valid_loader= DataLoader(train_data,batch_size=batch_size,\n",
    "                             num_workers=num_workers, \n",
    "                             sampler=valid_sampler)\n",
    "    \n",
    "    return train_loader, valid_loader , test_loader\n",
    "#     return train_loader , test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data firt for data normalization\n",
    "batch_size=50\n",
    "trian_loader= DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader= DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28]) tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
      "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
      "        9, 3])\n",
      "torch.float32\n",
      "tensor([0.1307]) tensor([0.1632])\n",
      "tensor([0.1325]) tensor([0.1642])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "idx=iter(trian_loader)\n",
    "imgs,target=next(idx)\n",
    "# by this method we can get the first batch whic in each bath we have(50 images)\n",
    "#based on the batch size\n",
    "print(imgs.shape ,target)\n",
    "print(imgs[0].dtype)\n",
    "\n",
    "\n",
    "# calculate the mean and standard varience of whole dataset\n",
    "\n",
    "# I m not sure about calculation of total standard deviation of dataset  at following dataset !!!\n",
    "def calculate_img_stats(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for imgs,_ in loader:\n",
    "        batch_samples = imgs.size(0)\n",
    "        mean += (imgs.mean(2).mean(2).sum(0))\n",
    "        std  += (imgs.std(3).std(2).sum(0))\n",
    "        nb_samples += batch_samples\n",
    "    return mean/nb_samples ,std/nb_samples\n",
    "\n",
    "mean,std=calculate_img_stats(trian_loader)\n",
    "print(mean,std)\n",
    "\n",
    "mean_test,std_test=calculate_img_stats(test_loader)\n",
    "print(mean_test,std_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normaliztion and augmentation on train data \n",
    "\n",
    "transform=transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                              transforms.RandomRotation(10),\n",
    "                              transforms.ToTensor(),transforms.Normalize([0.1307],[0.1632])])\n",
    "\n",
    "train_data=datasets.MNIST(root='MNIST_dataset',\n",
    "                          train=True,\n",
    "                          download=False,\n",
    "                          transform=transform)\n",
    "\n",
    "# data normalization on the test data\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                             transforms.Normalize([0.1325],[0.1642])])\n",
    "\n",
    "test_data=datasets.MNIST(root='MNIST_dataset',\n",
    "                          train=False,\n",
    "                          download=False,\n",
    "                          transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each bache of data has 50 images.\n",
      "Train_data:      960  baches \n",
      "Validation data: 240  baches \n",
      "Test_data:       200  baches\n"
     ]
    }
   ],
   "source": [
    "# split data to train, test and validation\n",
    "\n",
    "\n",
    "trian_loader, valid_loader, test_loader= split_image_data(train_data=train_data,\n",
    "                                                          test_data=test_data,\n",
    "                                                          valid_size=0.2,\n",
    "                                                         batch_size=batch_size)\n",
    "print('Each bache of data has {} images.'.format(batch_size))\n",
    "print('Train_data:      {}  baches \\nValidation data: {}  baches \\nTest_data:       {}  baches'.format(len(trian_loader),len(valid_loader),len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "def update_classwise_accuracies(preds,labels,class_correct,class_totals):\n",
    "    correct = np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
    "    for i in range(labels.shape[0]):\n",
    "        label = labels.data[i].item()\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_totals[label] += 1\n",
    "\n",
    "def get_accuracies(class_names,class_correct,class_totals):\n",
    "\n",
    "    accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "    class_accuracies = [(class_names[i],100.0*(class_correct[i]/class_totals[i]))\n",
    "                        for i in class_names.keys() if class_totals[i] > 0]\n",
    "    return accuracy,class_accuracies\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,device=None):\n",
    "        super().__init__()\n",
    "        if device is not None:\n",
    "                self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print('The Device is seted as: {}'.format(self.device))\n",
    "        self.best_accuracy = 0.\n",
    "            \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "    \n",
    "    def get_device(self):\n",
    "        return(self.device)\n",
    "        \n",
    "    def train_(self,trainloader,criterion,optimizer,print_every):\n",
    "        self.train()\n",
    "        t0 = time.time()\n",
    "        batches = 0\n",
    "        running_loss = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            batches += 1\n",
    "            #t1 = time.time()\n",
    "\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            #print('training this batch took {:.3f} seconds'.format(time.time() - t1))\n",
    "            running_loss += loss\n",
    "            if batches % print_every == 0:\n",
    "                print(f\"{time.asctime()}..\"\n",
    "                        f\"Time Elapsed = {time.time()-t0:.3f}..\"\n",
    "                        f\"Batch {batches+1}/{len(trainloader)}.. \"\n",
    "                        f\"Average Training loss: {running_loss/(batches):.3f}.. \"\n",
    "                        f\"Batch Training loss: {loss:.3f}.. \"\n",
    "                        )\n",
    "                t0 = time.time()\n",
    "                return running_loss/len(trainloader)\n",
    "            \n",
    "    def validate_(self,validloader):\n",
    "        running_loss = 0.\n",
    "        accuracy = 0\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, preds = torch.max(torch.exp(outputs), 1)\n",
    "                update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "                accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "                self.train()\n",
    "                return (running_loss/len(validloader),accuracy)\n",
    "\n",
    "\n",
    "    def evaluate(self,testloader):\n",
    "        self.eval()\n",
    "        self.model.to(self.device)\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                ps = torch.exp(outputs)\n",
    "                _, preds = torch.max(ps, 1)\n",
    "                update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "\n",
    "        self.train()\n",
    "        return get_accuracies(self.class_names,class_correct,class_totals)\n",
    "\n",
    "\n",
    "    def predict(self,inputs,topk=1):\n",
    "        self.eval()\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.forward(inputs)\n",
    "            ps = torch.exp(outputs)\n",
    "            p,top = ps.topk(topk, dim=1)\n",
    "        return p,top\n",
    "\n",
    "    def fit(self,trainloader,validloader,epochs=2,print_every=10,validate_every=1):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.to(self.device)\n",
    "            print('epoch {:3d}/{}'.format(epoch+1,epochs))\n",
    "            epoch_train_loss =  self.train_(trainloader,self.criterion,\n",
    "                                            self.optimizer,print_every)\n",
    "\n",
    "            if  validate_every and (epoch % validate_every == 0):\n",
    "                t2 = time.time()\n",
    "                epoch_validation_loss,epoch_accuracy = self.validate_(validloader)\n",
    "                time_elapsed = time.time() - t2\n",
    "                print(f\"{time.asctime()}--Validation time {time_elapsed:.3f} seconds..\"\n",
    "                      f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Epoch Training loss: {epoch_train_loss:.3f}.. \"\n",
    "                      f\"Epoch validation loss: {epoch_validation_loss:.3f}.. \"\n",
    "                      f\"validation accuracy: {epoch_accuracy:.3f}\")\n",
    "                if self.best_accuracy == 0. or (epoch_accuracy > self.best_accuracy):\n",
    "                    print('updating best accuracy: previous best = {:.3f} new best = {:.3f}'.format(self.best_accuracy,\n",
    "                                                                                     epoch_accuracy))\n",
    "                self.best_accuracy = epoch_accuracy\n",
    "                torch.save(self.state_dict(),self.best_accuracy_file)\n",
    "\n",
    "                self.train() # just in case we forgot to put the model back to train mode in validate\n",
    "\n",
    "        print('loading best accuracy model')\n",
    "        self.load_state_dict(torch.load(self.best_accuracy_file))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def set_criterion(self,criterion_name):\n",
    "        if criterion_name.lower() == 'nllloss':\n",
    "            self.criterion_name = 'NLLLoss'\n",
    "            self.criterion = nn.NLLLoss()\n",
    "        elif criterion_name.lower() == 'crossentropyloss':\n",
    "            self.criterion_name = 'CrossEntropyLoss'\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_optimizer(self,params,optimizer_name='adam',lr=0.003):\n",
    "        from torch import optim\n",
    "\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            print('setting optim Adam')\n",
    "            self.optimizer = optim.Adam(params,lr=lr)\n",
    "            self.optimizer_name = optimizer_name\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            print('setting optim SGD')\n",
    "            self.optimizer = optim.SGD(params,lr=lr)\n",
    "\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            print('setting optim Ada Delta')\n",
    "            self.optimizer = optim.Adadelta(params)\n",
    "\n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr, # learning rate\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         class_names):\n",
    "\n",
    "        self.set_criterion(criterion_name)\n",
    "        self.set_optimizer(self.parameters(),optimizer_name,lr=lr)\n",
    "        self.lr = lr\n",
    "        self.dropout_p = dropout_p\n",
    "        self.model_name =  model_name\n",
    "        self.best_accuracy = best_accuracy\n",
    "        self.best_accuracy_file = best_accuracy_file\n",
    "        self.class_names = class_names\n",
    "\n",
    "    def get_model_params(self):\n",
    "        params = {}\n",
    "        params['device'] = self.device\n",
    "#         params['model_name'] = self.model_name\n",
    "        params['optimizer_name'] = self.optimizer_name\n",
    "        params['criterion_name'] = self.criterion_name\n",
    "        params['lr'] = self.lr\n",
    "        params['dropout_p'] = self.dropout_p\n",
    "        params['best_accuracy'] = self.best_accuracy\n",
    "        params['best_accuracy_file'] = self.best_accuracy_file\n",
    "        params['class_names'] = self.class_names\n",
    "        return params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Device is seted as: cuda\n"
     ]
    }
   ],
   "source": [
    "A1=Network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tensor(x):\n",
    "    return x.view(x.shape[0],-1)\n",
    "\n",
    "class FC(Network):\n",
    "    def __init__(self,num_inputs,\n",
    "                 num_outputs,\n",
    "                 layers=[],\n",
    "                 lr=0.003,\n",
    "                 class_names=None,\n",
    "                 optimizer_name='Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 non_linearity='relu',\n",
    "                 criterion_name='NLLLoss',\n",
    "                 model_type='classifier',\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file.pth',\n",
    "                 device=None):\n",
    "\n",
    "        super().__init__(device=device)\n",
    "        self.get_model_params()\n",
    "        \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              'FC',\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        self.non_linearity = non_linearity\n",
    "        self.model = nn.Sequential()\n",
    "        if len(layers) > 0:\n",
    "            self.model.add_module('fc1',nn.Linear(num_inputs,layers[0]))\n",
    "            self.model.add_module('relu1',nn.ReLU())\n",
    "            self.model.add_module('dropout1',nn.Dropout(p=dropout_p,inplace=True))\n",
    "\n",
    "            for i in range(1,len(layers)):\n",
    "                self.model.add_module('fc'+str(i+1),nn.Linear(layers[i-1],layers[i]))\n",
    "                self.model.add_module('relu'+str(i+1),nn.ReLU())\n",
    "                self.model.add_module('dropout'+str(i+1),nn.Dropout(p=dropout_p))\n",
    "                self.model.add_module('out',nn.Linear(layers[-1],num_outputs))\n",
    "\n",
    "        else:\n",
    "            self.model.add_module('out',nn.Linear(num_inputs,num_outputs))\n",
    "            if model_type.lower() == 'classifier' and criterion_name.lower() == 'nllloss':\n",
    "                self.model.add_module('logsoftmax',nn.LogSoftmax(dim=1))\n",
    "                self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layer_dims = layers\n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        else:\n",
    "            self.class_names = {str(k):v for k,v in enumerate(list(range(num_outputs)))}\n",
    "            def forward(self,x):\n",
    "                return self.model(flatten_tensor(x))\n",
    "            \n",
    "    def _get_dropout(self):\n",
    "        for layer in self.model:\n",
    "            if type(layer) == torch.nn.modules.dropout.Dropout:\n",
    "                return layer.p\n",
    "\n",
    "    def _set_dropout(self,p=0.2):\n",
    "        for layer in self.model:\n",
    "            if type(layer) == torch.nn.modules.dropout.Dropout:\n",
    "                print('FC: setting dropout prob to {:.3f}'.format(p))\n",
    "                layer.p=p\n",
    "                \n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr,\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         model_type,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         chkpoint_file,\n",
    "                         num_inputs,\n",
    "                         num_outputs,\n",
    "                         layers,\n",
    "                         class_names):\n",
    "        super(FC, self).set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              model_name,\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layer_dims = layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        else:\n",
    "            self.class_names = {k:str(v) for k,v in enumerate(list(range(num_outputs)))}\n",
    "\n",
    "    def get_model_params(self):\n",
    "        params = super(FC, self).get_model_params()\n",
    "        params['num_inputs'] = self.num_inputs\n",
    "        params['num_outputs'] = self.num_outputs\n",
    "        params['layers'] = self.layer_dims\n",
    "        params['model_type'] = self.model_type\n",
    "        params['class_names'] = self.class_names\n",
    "        params['device'] = self.device\n",
    "        return params\n",
    "    \n",
    "    def load_chkpoint(chkpoint_file):\n",
    "\n",
    "        restored_data = torch.load(chkpoint_file)\n",
    "\n",
    "        params = restored_data['params']\n",
    "        print('load_chkpoint: best accuracy = {:.3f}'.format(params['best_accuracy']))\n",
    "\n",
    "        if params['model_type'].lower() == 'classifier':\n",
    "            net = FC( num_inputs=params['num_inputs'],\n",
    "                      num_outputs=params['num_outputs'],\n",
    "                      layers=params['layers'],\n",
    "                      device=params['device'],\n",
    "                      criterion_name = params['criterion_name'],\n",
    "                      optimizer_name = params['optimizer_name'],\n",
    "#                       model_name = params['model_name'],\n",
    "                      lr = params['lr'],\n",
    "                      dropout_p = params['dropout_p'],\n",
    "                      best_accuracy = params['best_accuracy'],\n",
    "                      best_accuracy_file = params['best_accuracy_file'],\n",
    "                      chkpoint_file = params['chkpoint_file'],\n",
    "                      class_names =  params['class_names']\n",
    "              )\n",
    "\n",
    "        net.load_state_dict(torch.load(params['best_accuracy_file']))\n",
    "\n",
    "        net.to(params['device'])\n",
    "\n",
    "        return net\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Device is seted as: cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FC' object has no attribute 'optimizer_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-074eee49d0a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0moptimizer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adadelta'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbest_accuracy_file\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'best_accuracy_mnist_fc_test.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           chkpoint_file='chkpoint_file_mnist_fc_test.pth')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-140-76e8d049b94a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_inputs, num_outputs, layers, lr, class_names, optimizer_name, dropout_p, non_linearity, criterion_name, model_type, best_accuracy, best_accuracy_file, chkpoint_file, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         self.set_model_params(criterion_name,\n",
      "\u001b[0;32m<ipython-input-140-76e8d049b94a>\u001b[0m in \u001b[0;36mget_model_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_inputs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_outputs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-1a82414b7306>\u001b[0m in \u001b[0;36mget_model_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m#         params['model_name'] = self.model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'criterion_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FC' object has no attribute 'optimizer_name'"
     ]
    }
   ],
   "source": [
    "net =  FC(num_inputs=784,\n",
    "          num_outputs=10,\n",
    "          layers=[512,512],\n",
    "          optimizer_name='Adadelta',\n",
    "          best_accuracy_file ='best_accuracy_mnist_fc_test.pth',\n",
    "          chkpoint_file='chkpoint_file_mnist_fc_test.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
