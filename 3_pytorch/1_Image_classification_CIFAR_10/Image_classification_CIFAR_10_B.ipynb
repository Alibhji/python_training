{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ali Babolhavaeji \n",
    "# 7/5/2019\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from torch.utils.data import *\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data\n",
    "def split_image_data(train_data,\n",
    "                     test_data=None,\n",
    "                     batch_size=20,\n",
    "                     num_workers=0,\n",
    "                     valid_size=0.2,\n",
    "                     sampler=SubsetRandomSampler):\n",
    "    \n",
    "    num_train=len(train_data)\n",
    "    indices= list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split=int(np.floor(valid_size*num_train))\n",
    "#     print(split)\n",
    "    train_idx,valid_idx=indices[split:],indices[:split]\n",
    "    train_sampler=sampler(train_idx)\n",
    "    valid_sampler=sampler(valid_idx)\n",
    "    \n",
    "    if test_data is not None:\n",
    "        test_loader= DataLoader(test_data, batch_size=batch_size,\n",
    "                                num_workers=num_workers) \n",
    "    else:\n",
    "        train_idx,test_idx= train_idx[split:],train_idx[:split]\n",
    "        train_sampler=sampler(train_idx)\n",
    "        test_sampler =sampler(test_idx)\n",
    "\n",
    "        test_loader= DataLoader(train_data, batch_size=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 sampler=test_sampler)\n",
    "        \n",
    "    train_loader= DataLoader(train_data, batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             sampler=train_sampler)\n",
    "    \n",
    "    valid_loader= DataLoader(train_data,batch_size=batch_size,\n",
    "                             num_workers=num_workers, \n",
    "                             sampler=valid_sampler)\n",
    "    \n",
    "    return train_loader, valid_loader , test_loader\n",
    "#     return train_loader , test_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "cifar10_mean = [0.4915, 0.4823, 0.4468]\n",
    "cifar10_std  = [0.2470, 0.2435, 0.2616] \n",
    "train_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "                                    ])\n",
    "\n",
    "train_data = datasets.CIFAR10('Cifar10', train=True,\n",
    "                              download=False, transform=train_transform)\n",
    "test_data = datasets.CIFAR10('Cifar10', train=False,\n",
    "                             download=False, transform=test_transform)\n",
    "\n",
    "trainloader,validloader,testloader = split_image_data(train_data,test_data,batch_size=batch_size)\n",
    "\n",
    "len(trainloader),len(testloader),len(validloader)\n",
    "\n",
    "# toTensor() converts a numpy array to a PyTorch Tensor (all our images are constructed as numpy arrays by the \n",
    "# dataset class when read from disk).\n",
    "# normalize() is a transform that normalizes according to the passed values\n",
    "# of means and STD of each channel as separate lists or tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the core classes to implement a basic neural network by using Pytorch\n",
    "import time as time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def update_classwise_accuracies(preds,labels,class_correct,class_totals):\n",
    "    correct=np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
    "    for i in range(lables.shape[0]):\n",
    "        label=label.data[i].item()\n",
    "        class_correct[label] += correct[i]+item()\n",
    "        class_totals[label] +=1\n",
    "        \n",
    "def get_accuracies(class_names,class_correct,class_totals):\n",
    "\n",
    "    accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "    class_accuracies = [(class_names[i],100.0*(class_correct[i]/class_totals[i]))\n",
    "                        for i in class_names.keys() if class_totals[i] > 0]\n",
    "    return accuracy,class_accuracies\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,device=None):\n",
    "        super().__init__()\n",
    "        if device is not None:\n",
    "            self.device=device\n",
    "        else:\n",
    "            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.best_accuracy = 0.\n",
    "            \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "\n",
    "                \n",
    "    def train_(self,trainloader,criterion,optimizer,print_every):\n",
    "        self.train()\n",
    "        t0=time.time()\n",
    "        batches = 0\n",
    "        running_loss = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            batches += 1\n",
    "            #t1 = time.time()\n",
    "\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            #print('training this batch took {:.3f} seconds'.format(time.time() - t1))\n",
    "            running_loss += loss\n",
    "            if batches % print_every == 0:\n",
    "                print(f\"{time.asctime()}..\"\n",
    "                        f\"Time Elapsed = {time.time()-t0:.3f}..\"\n",
    "                        f\"Batch {batches+1}/{len(trainloader)}.. \"\n",
    "                        f\"Average Training loss: {running_loss/(batches):.3f}.. \"\n",
    "                        f\"Batch Training loss: {loss:.3f}.. \"\n",
    "                        )\n",
    "                t0 = time.time()\n",
    "                return running_loss/len(trainloader)\n",
    "            \n",
    "            \n",
    "    def validate_(self,validloader):\n",
    "        running_loss = 0.\n",
    "        accuracy = 0\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, preds = torch.max(torch.exp(outputs), 1)\n",
    "                update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "                accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "                self.train()\n",
    "                return (running_loss/len(validloader),accuracy)\n",
    "            \n",
    "        def evaluate(self,testloader):\n",
    "            self.eval()\n",
    "            self.model.to(self.device)\n",
    "            class_correct = defaultdict(int)\n",
    "            class_totals = defaultdict(int)\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.forward(inputs)\n",
    "                    ps = torch.exp(outputs)\n",
    "                    _, preds = torch.max(ps, 1)\n",
    "                    update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "\n",
    "            self.train()\n",
    "            return get_accuracies(self.class_names,class_correct,class_totals)\n",
    "       \n",
    "    \n",
    "        def predict(self,inputs,topk=1):\n",
    "            self.eval()\n",
    "            self.model.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                ps = torch.exp(outputs)\n",
    "                p,top = ps.topk(topk, dim=1)\n",
    "            return p,top\n",
    "\n",
    "    def fit(self,trainloader,validloader,epochs=2,print_every=10,validate_every=1):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.to(self.device)\n",
    "            print('epoch {:3d}/{}'.format(epoch+1,epochs))\n",
    "            epoch_train_loss =  self.train_(trainloader,self.criterion,\n",
    "                                            self.optimizer,print_every)\n",
    "\n",
    "            if  validate_every and (epoch % validate_every == 0):\n",
    "                t2 = time.time()\n",
    "                epoch_validation_loss,epoch_accuracy = self.validate_(validloader)\n",
    "                time_elapsed = time.time() - t2\n",
    "                print(f\"{time.asctime()}--Validation time {time_elapsed:.3f} seconds..\"\n",
    "                      f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Epoch Training loss: {epoch_train_loss:.3f}.. \"\n",
    "                      f\"Epoch validation loss: {epoch_validation_loss:.3f}.. \"\n",
    "                      f\"validation accuracy: {epoch_accuracy:.3f}\")\n",
    "                if self.best_accuracy == 0. or (epoch_accuracy > self.best_accuracy):\n",
    "                    print('updating best accuracy: previous best = {:.3f} new best = {:.3f}'.format(self.best_accuracy,\n",
    "                                                                                     epoch_accuracy))\n",
    "                    self.best_accuracy = epoch_accuracy\n",
    "                    torch.save(self.state_dict(),self.best_accuracy_file)\n",
    "\n",
    "                self.train() # just in case we forgot to put the model back to train mode in validate\n",
    "\n",
    "        print('loading best accuracy model')\n",
    "        self.load_state_dict(torch.load(self.best_accuracy_file))\n",
    "\n",
    "    def set_criterion(self,criterion_name):\n",
    "            if criterion_name.lower() == 'nllloss':\n",
    "                self.criterion_name = 'NLLLoss'\n",
    "                self.criterion = nn.NLLLoss()\n",
    "            elif criterion_name.lower() == 'crossentropyloss':\n",
    "                self.criterion_name = 'CrossEntropyLoss'\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def set_optimizer(self,params,optimizer_name='adam',lr=0.003):\n",
    "        from torch import optim\n",
    "\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            print('setting optim Adam')\n",
    "            self.optimizer = optim.Adam(params,lr=lr)\n",
    "            self.optimizer_name = optimizer_name\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            print('setting optim SGD')\n",
    "            self.optimizer = optim.SGD(params,lr=lr)\n",
    "\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            print('setting optim Ada Delta')\n",
    "            self.optimizer = optim.Adadelta(params)\n",
    "            \n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr, # learning rate\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         chkpoint_file):\n",
    "\n",
    "        self.criterion_name = criterion_name\n",
    "        self.set_criterion(criterion_name)\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.set_optimizer(self.parameters(),optimizer_name,lr=lr)\n",
    "        self.lr = lr\n",
    "        self.dropout_p = dropout_p\n",
    "        self.model_name =  model_name\n",
    "        self.best_accuracy = best_accuracy\n",
    "        print('set_model_params: best accuracy = {:.3f}'.format(self.best_accuracy))\n",
    "        self.best_accuracy_file = best_accuracy_file\n",
    "        self.chkpoint_file = chkpoint_file\n",
    "\n",
    "    def get_model_params(self):\n",
    "        params = {}\n",
    "        params['device'] = self.device\n",
    "        params['model_name'] = self.model_name\n",
    "        params['optimizer_name'] = self.optimizer_name\n",
    "        params['criterion_name'] = self.criterion_name\n",
    "        params['lr'] = self.lr\n",
    "        params['dropout_p'] = self.dropout_p\n",
    "        params['best_accuracy'] = self.best_accuracy\n",
    "        print('get_model_params: best accuracy = {:.3f}'.format(self.best_accuracy))\n",
    "        params['best_accuracy_file'] = self.best_accuracy_file\n",
    "        params['chkpoint_file'] = self.chkpoint_file\n",
    "        print('get_model_params: chkpoint file = {}'.format(self.chkpoint_file))\n",
    "        return params\n",
    "\n",
    "    def save_chkpoint(self):\n",
    "        saved_model = {}\n",
    "        saved_model['params'] = self.get_model_params()\n",
    "        torch.save(saved_model,self.chkpoint_file)\n",
    "        print('checkpoint created successfully in {}'.format(self.chkpoint_file))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Network):\n",
    "    def __init__(self,num_inputs,\n",
    "                     num_outputs,\n",
    "                     layers=[],\n",
    "                     lr=0.003,\n",
    "                     class_names=None,\n",
    "                     optimizer_name='Adam',\n",
    "                     dropout_p=0.2,\n",
    "                     non_linearity='relu',\n",
    "                     criterion_name='NLLLoss',\n",
    "                     model_type='classifier',\n",
    "                     best_accuracy=0.,\n",
    "                     best_accuracy_file ='best_accuracy.pth',\n",
    "                     chkpoint_file ='chkpoint_file.pth',\n",
    "                     device=None):\n",
    "        super().__init__(device=device)\n",
    "\n",
    "        self.set_model_params(criterion_name,\n",
    "                                  optimizer_name,\n",
    "                                  lr,\n",
    "                                  dropout_p,\n",
    "                                  'FC',\n",
    "                                  best_accuracy,\n",
    "                                  best_accuracy_file,\n",
    "                                  chkpoint_file\n",
    "                                  )\n",
    "        self.non_linearity = non_linearity\n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        if len(layers) > 0:\n",
    "            self.model.add_module('fc1',nn.Linear(num_inputs,layers[0]))\n",
    "            self.model.add_module('relu1',nn.ReLU())\n",
    "            self.model.add_module('dropout1',nn.Dropout(p=dropout_p,inplace=True))\n",
    "            for i in range(1,len(layers)):\n",
    "                self.model.add_module('fc'+str(i+1),nn.Linear(layers[i-1],layers[i]))\n",
    "                self.model.add_module('relu'+str(i+1),nn.ReLU())\n",
    "                self.model.add_module('dropout'+str(i+1),nn.Dropout(p=dropout_p)) \n",
    "            self.model.add_module('out',nn.Linear(layers[-1],num_outputs))\n",
    "\n",
    "                                                                                                      \n",
    "        else:\n",
    "            self.model.add_module('out',nn.Linear(num_inputs,num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='data',download=True,\n",
    "                            transform = transforms.transforms.ToTensor())\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.0839, 0.2038, 0.1042],[0.2537, 0.3659, 0.2798])\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.0839, 0.2038, 0.1042],[0.2537, 0.3659, 0.2798])\n",
    "                                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='data',download=False,train=True, transform = train_transform)\n",
    "test_dataset = datasets.MNIST(root='data',download=False,train=False,transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 240, 200)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader,validloader,testloader = split_image_data(train_dataset,test_dataset,batch_size=50)\n",
    "len(trainloader),len(validloader),len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-116-29a14200b22b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-116-29a14200b22b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    setting optim Ada Delta\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "net =  FC(num_inputs=784,\n",
    "          num_outputs=10,\n",
    "          layers=[512,512],\n",
    "          optimizer_name='Adadelta',\n",
    "          best_accuracy_file ='best_accuracy_mnist_fc_test.pth',\n",
    "          chkpoint_file ='chkpoint_file_mnist_fc_test.pth')\n",
    "setting optim Ada Delta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
